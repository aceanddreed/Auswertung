Looking at the evaluation results of the p-value tests, we realize that we did not reach a sufficient significance level for most of our expectations. In the following subsection, we will discuss the results in relation to each of our hypotheses. \par \medskip

Our first hypothesis of the \textit{Evaluation Study}, \textbf{H1 - Gradient Plots and Ambiguation Plots will perform better than just visualizing the mean value for the \textit{Probability Estimation} and \textit{Probability Comparison} tasks.}, needs to be \textbf{rejected}. Our reasoning behind this hypothesis is based on the fact that for these task types we are asking for probabilities at given points in time or ask the user to compare two probabilities. Visualizing the uncertainty intervals might help reading those probability values and therefore make an estimation easier. The smallest p-value regarding this hypothesis represents the error-rate of the \textit{Probability Comparison} with a value of 0.1954 in Table \ref{table:kruskal_all}. However, looking at the tables for pairwise comparison, as well as the boxplots, it seems that this statistical difference in error-rate only holds between the user group with Ambiguation Plots and the user group with visualized mean values. Between the user group with Gradient Plots and the user group with visualized mean values, there is a less noticeable difference. Furthermore, p-values for the other testing variables of the \textit{Probability Estimation} and \textit{Probability Comparison} sessions are quite high (above 0.5) in general. Hence, our hypothesis \textbf{H1} cannot be confirmed. \par \medskip

Our second hypothesis, \textbf{H2 - The visualization of means alone will result into a better and faster performance compared to Gradient Plots and Ambiguation Plots for the \textit{Average Comparison} tasks.}, seems to be \textbf{plausible} but cannot be confirmed with absolute confidence. For tasks of the \textit{Average Comparison} session, participants had to decide between two events which will finish sooner on average. So for these tasks, the mean value actually holds enough information for giving an answer. Showing all the uncertainty information might be unnecessary for the user and maybe just makes the task more complicated. Looking at the results, the error-rate as well as the needed task completion time for the user group having visualized mean values are clearly lower compared to the other user groups, and also the confidence in the given answers is noticeable higher. However, the evaluation by p-value tests does not provide a high enough significance level. While the p-value for task completion time is slightly below 0.1, the p-value for the error-rate is about 0.24. For absolute confidence, these values are not low enough. \par \medskip

Our third hypothesis, \textbf{H4 - For the \textit{Overlap Estimation} we expect all three user groups to have problems with solving these tasks.}, seems to be \textbf{plausible}, as p-values (around 0.8) suggest that all samples come from the same distribution, i.e. all user groups are having the same difficulties solving those tasks. Furthermore, the error-rates are about three times higher compared to the \textit{Probability Estimation} session, were we also ask for quantitative values for estimated probabilities. The error-rates of the \textit{Probability Comparison} and \textit{Average Comparison} cannot be compared directly in this matters, since users are choosing between two options there instead of specifying a quantitative value. \par \medskip

When it comes to statistical significance, we realize that a total of 30 participants, 10 persons per user group each, is not enough for a quantitative user study. The variance in our gathered results tends to be very high, as many people might not answer that precisely. Furthermore, misunderstandings as mentioned before regarding complementary probabilities also affect the variance. However, the difference in performance between the individual visualization types might be small compared to the variance. Hence, we would need a lot of people in order to come to a statistical significant conclusion. For now, we performed the \textit{Evaluation Study} under supervision, allowing the participants to ask us if there are any misunderstandings or ambiguities. In order to conduct our study with a much larger amount of people, we would need to make it more understandable and robust in order to lower the variance in the results and also allow for an unsupervised study. \par \medskip

Another aspect which probably has an influence on the results is the study length. We received feedback from some participants saying that the study, especially the \textit{Probability Estimation} session, was too long. A longer study consisting of more tasks might be tedious for the user and leads to them losing their focus and motivation. Hence, results might be biased by the participantâ€™s mood or concentration. On the other hand, too short studies will produce less results per participants, increasing the variance and therefore decreasing the stability of the results. Also, if the learning phase of a task type takes too long, this affects a shorter study more than a longer study. If we extend our work in the future and make a follow-up study, we need to be careful when choosing the number of tasks.